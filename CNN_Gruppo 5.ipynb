{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd0ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963",
   "display_name": "Python 3.9.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(y: np.ndarray, n_classes) -> np.ndarray:\n",
    "        \"\"\"Encode labels into one-hot representation\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        y : Target values. shape = [n_samples]\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        onehot : array, shape = (n_samples, n_labels)\n",
    "        \"\"\"\n",
    "        onehot : np.ndarray = np.zeros((n_classes, y.shape[0]))\n",
    "        for idx, val in enumerate(y.astype(int)):\n",
    "            onehot[val, idx] = 1.\n",
    "        return onehot.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### System ###\n",
    "checkpoint_str = \"test/cp_ep40_bsize100_adam.ckpt\"\n",
    "checkpoint_path = Path(checkpoint_str)\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Make sure not to overwrite existing models\n",
    "try:\n",
    "    if len(os.listdir(checkpoint_path)) > 0:\n",
    "        checkpoint_str += '_'\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "### CNN ###\n",
    "CLASSES_CNN = 10\n",
    "CHANNELS_CNN = 1\n",
    "BATCH_SIZE_CNN = 100\n",
    "EPOCHS_CNN = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and shape the dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train : np.ndarray = x_train.reshape(x_train.shape + (CHANNELS_CNN,))\n",
    "x_test : np.ndarray = x_test.reshape(x_test.shape + (CHANNELS_CNN,))\n",
    "y_train : np.ndarray = onehot(y_train, CLASSES_CNN)\n",
    "y_test : np.ndarray = onehot(y_test, CLASSES_CNN)\n",
    "\n",
    "# The first parameter of 'shape' is the number of samples.\n",
    "num_train_samples = x_train.shape[0]\n",
    "num_batches = int(num_train_samples / BATCH_SIZE_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model every epoch, only if the model has improved\n",
    "cp_callback = ModelCheckpoint(checkpoint_str,\n",
    "                              monitor='accuracy',\n",
    "                              save_best_only=True,\n",
    "                              mode='max',\n",
    "                              save_weights_only=False,\n",
    "                              save_freq='epoch',\n",
    "                              #save_freq=5*num_batches,\n",
    "                              verbose=2)\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir=log_dir,\n",
    "                                          histogram_freq=1)\n",
    "callbacks = [cp_callback, tb_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "          activation='relu', input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3),\n",
    "          activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3),\n",
    "          activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(32, activation='sigmoid'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(CLASSES_CNN, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the currently loaded model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(0.0009),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CompletedProcess(args='removelogs.bat', returncode=0)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "import subprocess\n",
    "# Clear the logs folder before running the fit again\n",
    "subprocess.run('removelogs.bat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "600/600 [==============================] - 43s 71ms/step - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.0268 - val_accuracy: 0.9946\n",
      "\n",
      "Epoch 00001: accuracy improved from -inf to 0.99767, saving model to training5\\cp_ep60_bsize100_adam.ckpt\n",
      "INFO:tensorflow:Assets written to: training5\\cp_ep60_bsize100_adam.ckpt\\assets\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 40s 67ms/step - loss: 0.0068 - accuracy: 0.9982 - val_loss: 0.0321 - val_accuracy: 0.9935\n",
      "\n",
      "Epoch 00002: accuracy improved from 0.99767 to 0.99817, saving model to training5\\cp_ep60_bsize100_adam.ckpt\n",
      "INFO:tensorflow:Assets written to: training5\\cp_ep60_bsize100_adam.ckpt\\assets\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 39s 66ms/step - loss: 0.0060 - accuracy: 0.9983 - val_loss: 0.0280 - val_accuracy: 0.9945\n",
      "\n",
      "Epoch 00003: accuracy improved from 0.99817 to 0.99830, saving model to training5\\cp_ep60_bsize100_adam.ckpt\n",
      "INFO:tensorflow:Assets written to: training5\\cp_ep60_bsize100_adam.ckpt\\assets\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 40s 66ms/step - loss: 0.0056 - accuracy: 0.9986 - val_loss: 0.0309 - val_accuracy: 0.9936\n",
      "\n",
      "Epoch 00004: accuracy improved from 0.99830 to 0.99857, saving model to training5\\cp_ep60_bsize100_adam.ckpt\n",
      "INFO:tensorflow:Assets written to: training5\\cp_ep60_bsize100_adam.ckpt\\assets\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 40s 66ms/step - loss: 0.0061 - accuracy: 0.9983 - val_loss: 0.0312 - val_accuracy: 0.9942\n",
      "\n",
      "Epoch 00005: accuracy did not improve from 0.99857\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 40s 66ms/step - loss: 0.0057 - accuracy: 0.9985 - val_loss: 0.0311 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00006: accuracy did not improve from 0.99857\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 40s 67ms/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 0.0320 - val_accuracy: 0.9942\n",
      "\n",
      "Epoch 00007: accuracy improved from 0.99857 to 0.99867, saving model to training5\\cp_ep60_bsize100_adam.ckpt\n",
      "INFO:tensorflow:Assets written to: training5\\cp_ep60_bsize100_adam.ckpt\\assets\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 42s 69ms/step - loss: 0.0057 - accuracy: 0.9984 - val_loss: 0.0309 - val_accuracy: 0.9942\n",
      "\n",
      "Epoch 00008: accuracy did not improve from 0.99867\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 40s 67ms/step - loss: 0.0058 - accuracy: 0.9984 - val_loss: 0.0292 - val_accuracy: 0.9947\n",
      "\n",
      "Epoch 00009: accuracy did not improve from 0.99867\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 40s 67ms/step - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.0311 - val_accuracy: 0.9948\n",
      "\n",
      "Epoch 00010: accuracy did not improve from 0.99867\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 40s 67ms/step - loss: 0.0058 - accuracy: 0.9985 - val_loss: 0.0290 - val_accuracy: 0.9939\n",
      "\n",
      "Epoch 00011: accuracy did not improve from 0.99867\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 40s 66ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.0268 - val_accuracy: 0.9939\n",
      "\n",
      "Epoch 00012: accuracy did not improve from 0.99867\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 39s 64ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 0.0293 - val_accuracy: 0.9944\n",
      "\n",
      "Epoch 00013: accuracy improved from 0.99867 to 0.99888, saving model to training5\\cp_ep60_bsize100_adam.ckpt\n",
      "INFO:tensorflow:Assets written to: training5\\cp_ep60_bsize100_adam.ckpt\\assets\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.0287 - val_accuracy: 0.9946\n",
      "\n",
      "Epoch 00014: accuracy did not improve from 0.99888\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.0286 - val_accuracy: 0.9943\n",
      "\n",
      "Epoch 00015: accuracy did not improve from 0.99888\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0061 - accuracy: 0.9982 - val_loss: 0.0284 - val_accuracy: 0.9940\n",
      "\n",
      "Epoch 00016: accuracy did not improve from 0.99888\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 38s 64ms/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 0.0289 - val_accuracy: 0.9948\n",
      "\n",
      "Epoch 00017: accuracy did not improve from 0.99888\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.0317 - val_accuracy: 0.9945\n",
      "\n",
      "Epoch 00018: accuracy did not improve from 0.99888\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0046 - accuracy: 0.9986 - val_loss: 0.0314 - val_accuracy: 0.9945\n",
      "\n",
      "Epoch 00019: accuracy did not improve from 0.99888\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0311 - val_accuracy: 0.9947\n",
      "\n",
      "Epoch 00020: accuracy did not improve from 0.99888\n"
     ]
    }
   ],
   "source": [
    "# Train the currently loaded model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=BATCH_SIZE_CNN,\n",
    "                    epochs=EPOCHS_CNN,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reusing TensorBoard on port 6006 (pid 13004), started 0:00:01 ago. (Use '!kill 13004' to kill it.)"
     },
     "metadata": {}
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss:  0.002153349108994007\nscore:  0.9995333552360535\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the training set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print('loss: ', score[0])\n",
    "print('score: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss:  0.03178893402218819\nscore:  0.9929999709129333\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('loss: ', score[0])\n",
    "print('score: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the data obtained by the last evaluation\n",
    "with open('log.txt', 'a') as file:\n",
    "    file.write('File name: ' + checkpoint_str + '\\n')\n",
    "    file.write('loss: ' + str(score[0]) + '\\n')\n",
    "    file.write('score: ' + str(score[1]) + '\\n')\n",
    "    file.write('accuracy: ')\n",
    "    file.write(str(history.history['accuracy'][-1]) + '\\n')\n",
    "    file.write('\\n')\n",
    "\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 26, 26, 64)        640       \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 26, 26, 64)        256       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 11, 11, 32)        18464     \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 11, 11, 32)        128       \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 3, 3, 32)          9248      \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 3, 3, 32)          128       \n_________________________________________________________________\nflatten (Flatten)            (None, 288)               0         \n_________________________________________________________________\ndropout (Dropout)            (None, 288)               0         \n_________________________________________________________________\ndense (Dense)                (None, 32)                9248      \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 32)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                330       \n=================================================================\nTotal params: 38,442\nTrainable params: 38,186\nNon-trainable params: 256\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the model and print a summary of its structure\n",
    "model = tf.keras.models.load_model('./training5/cp_ep40_bsize100_adam.ckpt')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}